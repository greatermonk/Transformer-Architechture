{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1tEIXzPeChK8wzCnis5VWU96jE1Tr6kYE",
      "authorship_tag": "ABX9TyOk5wfyGaOfxVaGrcIrH8A3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greatermonk/Transformer-Architechture/blob/main/Wine_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXc5pvD-6Fa8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from tensorflow.python import keras\n",
        "from IPython.display import display, HTML\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, losses, callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#DEFINE HYPERPARAMETERS\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 80\n",
        "EMBEDDING_DIM = 256\n",
        "K_DIM = 256\n",
        "N_HEADS = 2\n",
        "FF_DIM = 256\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1\n",
        "\n",
        "#LOAD THE WINE-REVIEWS DATASET FROM YOUR DIRECTORY\n",
        "with open('/content/drive/MyDrive/Transformers/winemag-data-130k-v2.json') as json_data:\n",
        "     wine_reviews = json.load(json_data)\n",
        "\n",
        "#FILTER THE DATASET\n",
        "filtered_dataset = [\"wine review : \" + x[\"country\"] + \" : \" + x[\"province\"] + \":\" + x[\"variety\"] + \" : \" + x[\"description\"] for x in wine_reviews if x[\"country\"] is not None and x[\"province\"] is not None and x[\"variety\"] is not None and x[\"description\"] is not None]\n",
        "\n",
        "\n",
        "def pad_punctuation(s):\n",
        "     s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
        "     s = re.sub(\" +\", \" \", s)\n",
        "     return s\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_dataset]\n",
        "text_dataset = tf.data.Dataset.from_tensor_slices(text_data).batch(BATCH_SIZE).shuffle(10000)\n",
        "\n",
        "#Text Vectorization Layer\n",
        "vectorize_layer = layers.TextVectorization(standardize=\"lower\", max_tokens=VOCAB_SIZE, output_mode=\"int\", output_sequence_length=MAX_LEN + 1)\n",
        "vectorize_layer.adapt(text_dataset)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "'''for idx, word in enumerate(vocab[:10]):\n",
        "     print(f\"{idx}: {word}\")'''\n",
        "\n",
        "#example_tokenized_sentence = vectorize_layer(text_data[10])\n",
        "#print(text_data[10])\n",
        "#print(example_tokenized_sentence.numpy())\n",
        "\n",
        "#Now, create the training set of recipies as well as the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "     text = tf.expand_dims(text, -1)\n",
        "     tokenized_sentence = vectorize_layer(text)\n",
        "     x = tokenized_sentence[:, :-1]\n",
        "     y = tokenized_sentence[:, 1:]\n",
        "     return x, y\n",
        "\n",
        "\n",
        "train_dataset = text_dataset.map(prepare_inputs)\n",
        "example_io = train_dataset.take(1).get_single_element()\n",
        "#print(example_io[0][0]) #example input\n",
        "#print()\n",
        "#print(example_io[1][0]) #example output(shifted left by one token)\n",
        "\n",
        "#define attention mask\n",
        "def casual_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "     i = tf.range(n_dest)[:, None]\n",
        "     j = tf.range(n_src)\n",
        "     m = i >= j - n_src + n_dest\n",
        "     mask = tf.cast(m, dtype)\n",
        "     mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "     mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype = tf.int32)], 0)\n",
        "     return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "#print(np.transpose(casual_attention_mask(1, 10, 10, tf.int32)[0]))\n",
        "\n",
        "#Define a Transformer Block\n",
        "@register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "     def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate = 0.1):\n",
        "          super(TransformerBlock, self).__init__()\n",
        "          self.num_heads = num_heads\n",
        "          self.key_dim = key_dim\n",
        "          self.embed_dim = embed_dim\n",
        "          self.ff_dim = ff_dim\n",
        "          self.dropout_rate = dropout_rate\n",
        "          self.attention = layers.MultiHeadAttention(num_heads, key_dim, output_shape = embed_dim)\n",
        "          self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "          self.layerNormalizationLayer1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "          self.linearLayer1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "          self.linearLayer2 = layers.Dense(self.embed_dim)\n",
        "          self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "          self.layerNormalizationLayer2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "     def call(self, inputs):\n",
        "          input_shape = tf.shape(inputs)\n",
        "          batch_size = input_shape[0]\n",
        "          sequence_length = input_shape[1]\n",
        "          casual_mask = casual_attention_mask(batch_size, sequence_length, sequence_length, tf.bool)\n",
        "          attn_output, attn_scores = self.attention(inputs, inputs, attention_mask = casual_mask, return_attention_scores = True)\n",
        "          attn_output = self.dropout_1(attn_output)\n",
        "          out1 = self.layerNormalizationLayer1(inputs + attn_output)\n",
        "          ffn_1 = self.linearLayer1(out1)\n",
        "          ffn_2 = self.linearLayer2(ffn_1)\n",
        "          ffn_output = self.dropout_2(ffn_2)\n",
        "          return self.layerNormalizationLayer2(out1 + ffn_output), attn_scores\n",
        "\n",
        "\n",
        "     def get_config(self):\n",
        "          config = super().get_config()\n",
        "          config.update(\n",
        "               {\n",
        "                    \"key_dim\": self.key_dim,\n",
        "                    \"embed_dim\": self.embed_dim,\n",
        "                    \"num_heads\": self.num_heads,\n",
        "                    \"ff_dim\": self.ff_dim,\n",
        "                    \"dropout_rate\": self.dropout_rate\n",
        "               }\n",
        "          )\n",
        "          return config\n",
        "\n",
        "@register_keras_serializable()\n",
        "class TokenAndPositionalEmbedding(layers.Layer):\n",
        "     def __init__(self, max_len, vocab_size, embed_dim):\n",
        "          super(TokenAndPositionalEmbedding, self).__init__()\n",
        "          self.max_len = max_len\n",
        "          self.vocab_size = vocab_size\n",
        "          self.embed_dim = embed_dim\n",
        "          self.token_embedding = layers.Embedding(input_dim = vocab_size, output_dim = embed_dim)\n",
        "          self.positional_embedding = layers.Embedding(input_dim = max_len, output_dim = embed_dim)\n",
        "\n",
        "     def call(self, x):\n",
        "          max_length = tf.shape(x)[-1]\n",
        "          positions = tf.range(0, max_length, 1)\n",
        "          position_embeddings = self.positional_embedding(positions)\n",
        "          token_embeddings = self.token_embedding(x)\n",
        "          return token_embeddings + position_embeddings\n",
        "\n",
        "     def get_config(self):\n",
        "          config = super().get_config()\n",
        "          config.update(\n",
        "               {\n",
        "                \"max_length\": self.max_len,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim\n",
        "               }\n",
        "          )\n",
        "          return config\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape = (None, ), dtype = tf.int32)\n",
        "x = TokenAndPositionalEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x, attention_scores = TransformerBlock(N_HEADS, K_DIM, EMBEDDING_DIM, FF_DIM)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation = \"softmax\")(x)\n",
        "gpt_model = models.Model(inputs = inputs, outputs = [outputs, attention_scores])\n",
        "gpt_model.compile(\"adam\", loss = [losses.SparseCategoricalCrossentropy(), None], metrics = [\"acc\", \"precision\"])\n",
        "gpt_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "class TextGenerator(callbacks.Callback):\n",
        "     def __init__(self, index_to_word):\n",
        "          super(TextGenerator, self).__init__()\n",
        "          self.index_to_word = index_to_word\n",
        "          self.word_to_index = {word: index for index, word in enumerate(index_to_word)}\n",
        "\n",
        "\n",
        "     def sample_from(self, probs, temperature):\n",
        "          probs =  probs ** (1 / temperature)\n",
        "          probs = probs / np.sum(probs)\n",
        "          return np.random.choice(len(probs), p = probs), probs\n",
        "\n",
        "     def generate(self, start_prompt, max_tokens, temperature):\n",
        "          start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
        "          sample_token = None\n",
        "          info = []\n",
        "          while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "               x = np.array([start_tokens])\n",
        "               y, attn  = self.model.predict(x, verbose = 0)\n",
        "               sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "               info.append(\n",
        "                    {\n",
        "                         \"prompt\": start_prompt,\n",
        "                         \"word_probabilities\": probs,\n",
        "                         \"attention_scores\": attn[0, :, -1, :]\n",
        "                    }\n",
        "               )\n",
        "               start_tokens.append(sample_token)\n",
        "               start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "          print(f\"\\nGenerated text:\\n{start_prompt}\\n\")\n",
        "          return info\n",
        "\n",
        "     def on_epoch_end(self, epoch, logs=None):\n",
        "          super().on_epoch_end(epoch, logs)\n",
        "          self.generate(\"wine review\", max_tokens = 50, temperature = 1.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(filepath =  \"/.weights.h5\", save_weights_only=True, save_freq=\"epoch\", verbose = 0)\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir = \"./logs\")\n",
        "text_generator = TextGenerator(vocab)\n",
        "gpt_model.fit(train_dataset, epochs = EPOCHS, callbacks = [model_checkpoint_callback, tensorboard_callback, text_generator], batch_size=BATCH_SIZE)\n",
        "gpt_model.save('/content/drive/MyDrive/Transformers/gpt_model.keras')\n",
        "gpt_model.save_weights('/content/drive/MyDrive/Transformers/gpt_model_weights.weights.h5')\n",
        "\n",
        "\n",
        "def display_probs_graph(info, vocab, top_k=10):\n",
        "    for i in info:\n",
        "        # Display highlighted text\n",
        "        highlighted_text = []\n",
        "        for word, attn_score in zip(i[\"prompt\"].split(), np.mean(i[\"attention_scores\"], axis=0)):\n",
        "             highlighted_text.append(\n",
        "                  f\"<span style='background-color: rgba(155, 135, 12, {attn_score / max(np.mean(i['attention_scores'], axis=0))})'> \\t {word}</span>\"\n",
        "             )\n",
        "        highlighted_text = \" \".join(highlighted_text)\n",
        "        display(HTML(highlighted_text))\n",
        "        # Get top k probabilities and their corresponding words\n",
        "        word_probs = i[\"word_probabilities\"]\n",
        "        top_indices = np.argsort(word_probs)[::-1][:top_k]\n",
        "        top_probs = word_probs[top_indices]\n",
        "        top_words = [vocab[idx] for idx in top_indices]\n",
        "        # Create horizontal bar plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        plt.style.use(\"ggplot\")\n",
        "        y_pos = np.arange(len(top_words))\n",
        "        ax.barh(y_pos, top_probs, align='center')\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(top_words)\n",
        "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
        "        ax.set_xlabel('Probability')\n",
        "        ax.set_title('Top {} tokens and their probabilities'.format(top_k))\n",
        "        # Add probability values at the end of each bar\n",
        "        for j, v in enumerate(top_probs):\n",
        "            ax.text(v, j, f' {v:.2%}', va='center')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(\"---------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "info = text_generator.generate(\"wine_review: us\", max_tokens = 50, temperature = 1.0)\n",
        "#info2 = text_generator.generate(\"wine_review: italy\", max_tokens = 30, temperature = 0.7)\n",
        "#info3 = text_generator.generate(\"wine_review: germany\", max_tokens = 30, temperature = 0.5)\n",
        "display_probs_graph(info, vocab)\n",
        "print(\"\\n\")\n",
        "#display_probs_graph(info2, vocab)\n",
        "print(\"\\n\")\n",
        "#display_probs_graph(info3, vocab)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}